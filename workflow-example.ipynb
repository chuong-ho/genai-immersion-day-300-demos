{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ad94911",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerativeai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgenai\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load environment variables from .env file\u001b[39;00m\n\u001b[1;32m      8\u001b[0m load_dotenv()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get OpenAI API key\n",
    "openai_api_key = os.environ.get('openai_api_key')\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"‚ö†Ô∏è  openai_api_key not found in .env file\")\n",
    "else:\n",
    "    print(\"‚úì OpenAI API key loaded from .env file\")\n",
    "    \n",
    "# Get Gemini API key\n",
    "gemini_api_key = os.environ.get('gemini_api_key')\n",
    "\n",
    "if not gemini_api_key:\n",
    "    print(\"‚ö†Ô∏è  gemini_api_key not found in .env file\")\n",
    "    print(\"Please add: gemini_api_key=your-api-key to .env file\")\n",
    "else:\n",
    "    print(\"‚úì Gemini API key loaded from .env file\")\n",
    "    \n",
    "# Create OpenAI client\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=gemini_api_key)\n",
    "gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e371ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prompt from user and send to openai\n",
    "#user_prompt = input(\"Enter your prompt: \")\n",
    "user_prompt = \"Who am I talking to?\"\n",
    "\n",
    "print(f\"\\nüì§ Sending to openai: {user_prompt}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Send to openai\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",  # or \"gpt-3.5-turbo\" for faster/cheaper responses\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get the response\n",
    "ai_response = response.choices[0].message.content\n",
    "\n",
    "print(\"\\nüì• openai Response:\")\n",
    "print(ai_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Function to send prompts with system message\n",
    "def chat_with_openai(user_prompt, system_message=None, model=\"gpt-4\"):\n",
    "    \"\"\"\n",
    "    Send a prompt to openai and return the response\n",
    "    \n",
    "    Args:\n",
    "        user_prompt: The user's message/prompt\n",
    "        system_message: Optional system message to set context\n",
    "        model: The openai model to use (default: gpt-4)\n",
    "    \n",
    "    Returns:\n",
    "        The AI's response as a string\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    \n",
    "    # Add system message if provided\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    \n",
    "    # Add user prompt\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    \n",
    "    # Call openai API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "user_input = input(\"Ask me anything: \")\n",
    "system_context = \"You are a helpful assistant that provides concise answers.\"\n",
    "\n",
    "answer = chat_with_openai(user_input, system_message=system_context)\n",
    "print(f\"\\nü§ñ Response:\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179b8683",
   "metadata": {},
   "source": [
    "## Advanced: Gradio Chatbot with Conversation History\n",
    "\n",
    "This creates a chatbot interface that remembers your conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce61d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def dual_ai_response(message):\n",
    "    \"\"\"\n",
    "    Get OpenAI response and Gemini evaluation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get OpenAI response\n",
    "        openai_response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": message}]\n",
    "        )\n",
    "        openai_answer = openai_response.choices[0].message.content\n",
    "        \n",
    "        # Get Gemini evaluation\n",
    "        gemini_prompt = f\"\"\"Evaluate this AI response:\n",
    "\n",
    "Question: {message}\n",
    "Response: {openai_answer}\n",
    "\n",
    "Rate it (1-10) and explain why. Is it accurate and helpful?\"\"\"\n",
    "        \n",
    "        gemini_response = gemini_model.generate_content(gemini_prompt)\n",
    "        gemini_evaluation = gemini_response.text\n",
    "        \n",
    "        return openai_answer, gemini_evaluation\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", \"Unable to evaluate\"\n",
    "\n",
    "# Create custom interface with two output boxes\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# AI Response with Gemini Quality Check\")\n",
    "    gr.Markdown(\"Ask a question to OpenAI GPT-4, and Gemini will evaluate the response quality!\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            user_input = gr.Textbox(\n",
    "                label=\"Your Question\", \n",
    "                lines=3,\n",
    "                placeholder=\"Ask anything...\"\n",
    "            )\n",
    "            submit_btn = gr.Button(\"üöÄ Ask & Evaluate\", variant=\"primary\")\n",
    "        \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            openai_output = gr.Textbox(\n",
    "                label=\"ü§ñ OpenAI GPT-4 Response\", \n",
    "                lines=15,\n",
    "                show_copy_button=True\n",
    "            )\n",
    "        with gr.Column():\n",
    "            gemini_output = gr.Textbox(\n",
    "                label=\"‚ú® Gemini Quality Evaluation\", \n",
    "                lines=15,\n",
    "                show_copy_button=True\n",
    "            )\n",
    "    \n",
    "    # Example questions\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            \"Explain quantum computing in simple terms\",\n",
    "            \"Write a haiku about artificial intelligence\",\n",
    "            \"What is the difference between machine learning and deep learning?\",\n",
    "            \"How does photosynthesis work?\"\n",
    "        ],\n",
    "        inputs=user_input\n",
    "    )\n",
    "    \n",
    "    submit_btn.click(\n",
    "        fn=dual_ai_response,\n",
    "        inputs=user_input,\n",
    "        outputs=[openai_output, gemini_output]\n",
    "    )\n",
    "\n",
    "# Launch with public sharing enabled\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0691afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Gradio interface\n",
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
